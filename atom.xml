<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>ùîπlog ‚ô¶ ùïçal√©ria ‚Ñïic√©ria</title>
  
  <subtitle>Val√©ria Nic√©ria</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://valerianiceria.github.io/"/>
  <updated>2020-06-21T01:24:27.272Z</updated>
  <id>https://valerianiceria.github.io/</id>
  
  <author>
    <name>Val√©ria Nic√©ria</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Minera√ß√£o de texto</title>
    <link href="https://valerianiceria.github.io/2020/05/17/mineracao-texto/"/>
    <id>https://valerianiceria.github.io/2020/05/17/mineracao-texto/</id>
    <published>2020-05-17T15:53:59.000Z</published>
    <updated>2020-06-21T01:24:27.272Z</updated>
    
    <content type="html"><![CDATA[<p>Atualmente, vivemos na era do Big Data, ou seja, estamos gerando dados a todo momento, por√©m, na maioria das vezes, s√£o dados n√£o estruturados, como not√≠cias, e-mails e textos de forma geral. Minera√ß√£o de textos ou do ingl√™s <em>Text Mining</em>, tem como objetivo, encontrar termos relevantes e estabelecer relacionamento entre eles de acordo com a sua frequ√™ncia e assim extrair informa√ß√µes de grandes volumes de textos.</p><h2 id="Workflow"><a href="#Workflow" class="headerlink" title="Workflow"></a>Workflow</h2><p><img src="/images/mineracao-texto/ciclo_mineracao.png" alt="Ciclo de minera√ß√£o"></p><p>Agora, que sabemos que √© poss√≠vel obter informa√ß√µes, de grandes volumes de textos, vejamos como √© o processo de obten√ß√£o dessas informa√ß√µes:</p><ul><li><p><strong>Come√ßar com uma pergunta:</strong><br>Primeiramente, devemos ter um problema que queremos resolver, ou uma pergunta que desejamos responder, como, por exemplo: <em>Qual o pet mais querido no momento?</em></p></li><li><p><strong>Obter os dados:</strong><br>Agora, que temos um questionamento, precisamos conseguir os dados que o responda, sendo assim, utilizarei como fonte de dados, o que as pessoas est√£o conversando no Twitter.</p></li><li><p><strong>Limpar:</strong><br>E com os nossos dados em m√£os, iremos realizar outra etapa do processo, que √© a limpeza dos nossos dados, removendo caracteres especiais, como acentos, pontua√ß√µes, tranformando todas as palavras em uma s√≥ estrutura, como, min√∫sculo e removeremos todas as <strong>stopwords</strong>, que s√£o palavras irrelevantes para a pergunta que queremos responder.</p></li><li><p><strong>Analisar:</strong><br>Com os nossos dados prontos, iremos realizar uma das partes mais divertidas, que √© analisar os nossos dados, onde poderemos aplicar diversas t√©cnicas e verificar se com o dados que possu√≠mos, responderemos √† pergunta que nos motivou a analisar esses dados.</p></li><li><p><strong>Visualizar:</strong><br>Nessa etapa, poderemos visualizar o resultado da nossa an√°lise e assim gerar diversas op√ß√µes de gr√°ficos, como, por exemplo, nuvem de palavras.</p></li><li><p><strong>Extrair conhecimento:</strong><br>E chegamos a √∫ltima etapa, e se tudo estiver ocorrido bem, durante o processo de an√°lise, teremos transformado os nossos dados em informa√ß√£o e agregando ao nosso entendimento pr√©vio sobre o assunto, como resultado, gerado um conhecimento novo, sobre o fato que est√°vamos analisando.</p></li></ul><h2 id="Conceitos"><a href="#Conceitos" class="headerlink" title="Conceitos"></a>Conceitos</h2><p>Antes, de continuarmos, vamos conhecer alguns conceitos:</p><ul><li><strong>Corpus:</strong> Conjuntos de textos.</li><li><strong>Stopwords:</strong> Como comentado anteriormente, s√£o palavras que n√£o adicionam sentido ao texto, como palavras de liga√ß√£o por exemplo e existem listas de stopwords para v√°rios idiomas na internet.</li></ul><h2 id="Codigo-exemplo"><a href="#Codigo-exemplo" class="headerlink" title="C√≥digo exemplo"></a>C√≥digo exemplo</h2><p>Chegou o momento mais divertido onde criaremos um projeto b√°sico de text mining, e para isso, utilizaremos a linguagem de programa√ß√£o <a href="https://cran.r-project.org/">R</a> e os seguintes pacotes:</p><ul><li><em>‚Äòrtweet‚Äô</em> √â um pacote, que permitir√° que voc√™ se conecte ao Twitter, caso voc√™ tenha uma conta, onde voc√™ poder√° realizar buscas, com no m√°ximo 18 mil tweets.</li><li><em>‚Äòtm‚Äô</em> O pacote tm de ‚ÄúText Mining‚Äù √© um pacote utilizado para trabalharmos com textos.</li><li><em>‚Äòwordcloud‚Äô</em> √â um pacote que nos permite visualizar de forma r√°pida, as palavras, utilizando como crit√©rio de tamanho, a frequ√™ncia.</li><li><em>‚Äòtydeverse‚Äô</em> √â um pacote, que possui uma cole√ß√£o de pacotes inclusos, para ajudar na manipula√ß√£o dos dados.</li></ul><p>Primeiramente, vamos instalar os pacotes que ser√£o necess√°rios durante o projeto:</p><pre class=" language-R"><code class="language-R"># Instalando os pacotesinstall.packages("rtweet")install.packages("tm")install.packages("wordcloud")install.packages("tidyverse")</code></pre><p>E com os pacotes instalados, devemos carregar os mesmos e assim poderemos utilizar as fun√ß√µes desses pacotes.</p><pre class=" language-R"><code class="language-R"># Carregando os pacoteslibrary(tm)library(rtweet)library(wordcloud)library(tidyverse)</code></pre><p>Precisaremos de dados e vamos coletar esses dados utilizando a API do Twitter, usando a fun√ß√£o de busca <em>‚Äòsearch_tweets()‚Äô</em>, onde poderemos passar a <strong>#</strong> que queremos buscar ou termo, o n√∫mero de tweets, onde o n√∫mero m√°ximo √© 18 mil e se queremos ou n√£o os retweets e a linguagem dos tweets, que no nosso caso ser√° em ingl√™s.</p><pre class=" language-R"><code class="language-R"># Buscando os tweets com #pets ou #petpets_tweets <- search_tweets(  "#pets OR #pet",  n = 18000,  include_rts = FALSE,  lang = "en")</code></pre><p>Visualizando a frequ√™ncia de tweets utilizando #pets ou #pet, no intervalo de 1 hora:</p><pre class=" language-R"><code class="language-R"># Gerando um gr√°fico com a frequ√™ncia dos tweets no intervalo de 1 horapets_tweets %>%   ts_plot("1 hours") +  ggplot2::theme_minimal() +  ggplot2::theme(plot.title = ggplot2::element_text(face = "bold")) +  ggplot2::labs(    x = NULL,    y = NULL,    title = "Frequ√™ncia de Pets no Twitter",    subtitle = "Tweets no intervalo de 1 hora",    caption = "\nSource: Dados coletados no Twitter REST API via rteet"  )</code></pre><p><img src="/images/mineracao-texto/frequencia.png" alt="Gr√°fico de frequ√™ncia"></p><p>Vamos come√ßar a minera√ß√£o dos textos e para isso iremos pegar a (coluna) <em>text</em> e atribuir a uma vari√°vel.</p><pre class=" language-R"><code class="language-R"># Atribuindo os textos a uma vari√°velpet_text <- pets_tweets %>% pull(text)</code></pre><p>Tranformando os nossos textos em um corpus, para assim podermos realizar a limpeza utilizando a fun√ß√£o <em>tm_map</em>, onde removeremos os caracteres especiais, transformaremos todas as letras para min√∫sculas, removeremos as pontua√ß√µes e as stopwords em ingl√™s.</p><pre class=" language-R"><code class="language-R"># Transformando os textos em um corpuspet_corpus <- VCorpus(VectorSource(pet_text))# Realizando a limpeza dos dados e removendo os termos 'pet' e 'pets', # pois √© √≥bvio que essas palavras est√£o na nossa busca e ser√£o as mais frequentespet_corpus <-   tm_map(    pet_corpus,    content_transformer(      function(x) iconv(x, from = 'UTF-8', to = 'ASCII//TRANSLIT')    )  ) %>%   tm_map(content_transformer(tolower)) %>%   tm_map(removePunctuation) %>%   tm_map(removeWords, stopwords("english")) %>%   tm_map(removeWords, c("pet", "pets"))</code></pre><p>Ap√≥s, realizar a limpeza dos nossos textos, chegou o momento de visualizar o resultado em uma nuvem de palavras e iremos utilizar a fun√ß√£o <em>brewer.pal</em>, para gerar as cores em hexadecimal, para assim, colorirmos a nossa nuvem.</p><pre class=" language-R"><code class="language-R"># Lista de cores em hexadecimalpaleta <- brewer.pal(8, "Dark2")# Criando uma nuvem de palavras, com no m√°ximo 100 palavras# onde tenha se repetido ao menos 2 vezeswordcloud(  pet_corpus,  min.freq = 2,  max.words = 100,  color = paleta)</code></pre><p><img src="/images/mineracao-texto/nuvem1.png" alt="Nuvem de palavras"></p><p>Criando uma matriz de documentos-termos <em>(DocumentTermMatrix)</em>, onde posteriormente, removeremos os termos <em>menos</em> frequentes da matriz e somaremos os termos restantes para assim verificar quais s√£o os termos mais utilizados.</p><pre class=" language-R"><code class="language-R"># Criando uma matriz de termospets_document <- DocumentTermMatrix(pet_corpus)# Removendo os termos menos frequentespets_doc <- removeSparseTerms(pets_document, 0.98)# Gerando uma matrix ordenada, com o termos mais frequentespets_freq <-   pets_doc %>%   as.matrix() %>%   colSums() %>%   sort(decreasing = T)</code></pre><p>Gerando um dataframe com os termos mais utilizados e visualizando em um gr√°fico.</p><pre class=" language-R"><code class="language-R"># Criando um dataframe com as palavras mais frequentesdf_pets <- data.frame(  word = names(pets_freq),  freq = pets_freq)# Gerando um gr√°fico da frequ√™nciadf_pets %>%  subset(freq > 450) %>%   ggplot(aes(x = reorder(word, freq),             y = freq)) +  geom_bar(stat = "identity", fill='#0c6cad', color="#075284") +  theme(axis.text.x = element_text(angle = 45, hjus = 1)) +  ggtitle("Termos relacionados a Pet ou Pets mais frequentes no Twitter") +  labs(y = "Frequ√™ncia", x = "Termos") +  coord_flip()</code></pre><p><img src="/images/mineracao-texto/termos_freq.png" alt="Termos mais frequentes"></p><p>E podemos visualizar o resultado em uma nuvem de palavras, por√©m utilizaremos outro pacote para gerar a nuvem que √© o <em>wordcloud2</em>, pois ele gera uma nuvem de palavras mais bonita que o pacote que utilizamos at√© o momento,  mas antes, temos que instalar o pacote em nosso computador e utilizamos o comando abaixo para realizar a instala√ß√£o.</p><pre class=" language-R"><code class="language-R"># Instalando o pacote 'devtools', caso n√£o o tenha instalado em seu computadorif (!require(devtools)) install.packages("devtools")# Carregando o pacote 'devtools'library(devtools)# Instalando o pacote 'wordcloud2' via githubdevtools::install_github("lchiffon/wordcloud2", force = TRUE)</code></pre><p>E ap√≥s, instalarmos, carregaremos o pacote <em>wordcloud2</em> e passaremos o nosso dataframe com os termos mais frequentes para a fun√ß√£o <em>wordcloud2</em> e teremos como resultado o seguinte gr√°fico.</p><pre class=" language-R"><code class="language-R"># Carregando o pacote 'wordcloud2'library(wordcloud2)wordcloud2(data = df_pets)</code></pre><p><img src="/images/mineracao-texto/nuvem2.png" alt="Nuvem de palavras"></p><p>E podemos visualizar como os nossos termos est√£o relacionados, e para isso produziremos um dendrograma de agrupamento hier√°rquico, que √© um diagrama de √°rvore.</p><pre class=" language-R"><code class="language-R"># Removendo os termos menos frequentespets_doc1 <- removeSparseTerms(pets_document, 0.95)# Dendogramadistancia <- dist(t(pets_doc1), method = "euclidian")dendograma <- hclust(d = distancia, method = "complete")plot(dendograma, habg = -1, main = "Dendograma Tweets Pets ou Pet",     xlab = "Dist√¢ncia",     ylab = "Altura")</code></pre><p><img src="/images/mineracao-texto/dendograma.png" alt="Dendograma"></p><p>E outra possibilidade que temos ao realizar minera√ß√£o de textos √© classificar os sentimentos dos termos utilizados, no nosso caso n√£o far√° muito sentido, pois √© √≥bvio que o sentimento das pessoas em rela√ß√£o aos pets √© um sentimento positivo, mas somente para exemplificar a posibilidade, vamos realizar essa an√°lise e para isso utilizaremos o pacote <em>syuzhet</em>.</p><pre class=" language-R"><code class="language-R"># Instalando o pacote, caso n√£o o tenha em seu PC.install.packages("syuzhet")# Carregando o pacotelibrary(syuzhet)</code></pre><p>Agora, que instalamos e carregamos o pacote, realizaremos a an√°lise dos sentimentos dos nossos tweets e para tal an√°lise utilizaremos a fun√ß√£o <em>get_nrc_sentiment</em>, onde passaremos como par√¢metro, os termos da nossa matriz de documentos-termos. E ap√≥s, obtermos as emo√ß√µes dos nossos termos, faremos o calculo da frequ√™ncia dos sentimentos que utilizaram a #pet ou #pets.</p><pre class=" language-R"><code class="language-R"># Obtendo as emo√ß√µes dos nossos termospets_sentimentos <- get_nrc_sentiment(  pets_doc$dimnames$Terms,  language = "english")# Calculando a frequ√™ncia dos sentimentospets_sentimentos_freq <- pets_sentimentos %>%  colSums() %>%   sort(decreasing = T)</code></pre><p>Com a frequ√™ncia dos nossos sentimentos calculada, poderemos visualizar o resultado, mas antes, iremos traduzir os sentimentos do ingl√™s para o portugu√™s e tranformar o resultado em um dataframe para posterirmente gerarmos o gr√°fico.</p><pre class=" language-R"><code class="language-R"># Criando um dataframe com os sentimentos traduzidos, que ser√° utilizado como de-para. sentimetos_traducao <-   data.frame(    sentiment = c(      "positive",      "negative",      "trust",      "anticipation",      "fear",      "joy",      "sadness",      "surprise",      "anger",      "disgust"    ),    sentimentos = c(      "Positivo",      "Negativo",      "Confian√ßa",      "Antecipa√ß√£o",      "Medo",      "Alegria",      "Tristeza",      "Surpresa",      "Raiva",      "Nojo"    )  )# Tranformando os resultados da frequ√™ncia em um dataframe # e juntando ao dataframe de tradu√ß√£odf_sentimento <-   data.frame(    sentiment = names(pets_sentimentos_freq),    freq = pets_sentimentos_freq  ) %>%   left_join(sentimetos_traducao, by = "sentiment") %>%   dplyr::select(-sentiment) %>%   arrange(desc(freq))# Visualizando a frequ√™ncia dos sentimentos em rela√ß√£o a #petsggplot(data = df_sentimento,       aes(x = reorder(sentimentos, -freq), y = freq)) +  geom_bar(aes(fill=sentimentos), stat = "identity") +  theme(legend.position = "none",        axis.text.x = element_text(angle = 45, hjus = 1)) +  xlab("Sentimentos") +  ylab("Frequ√™ncia") +  ggtitle("Sentimentos das pessoas em rela√ß√£o aos Pets")</code></pre><p><img src="/images/mineracao-texto/sentimento.png" alt="Gr√°fico de sentimentos"></p><p>Ent√£o, √© isso, espero que tenha gostado desse post e qual o termo que voc√™ utilizaria para buscar no Twitter?</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Atualmente, vivemos na era do Big Data, ou seja, estamos gerando dados a todo momento, por√©m, na maioria das vezes, s√£o dados n√£o estrutu
      
    
    </summary>
    
    
      <category term="Data Science" scheme="https://valerianiceria.github.io/categories/Data-Science/"/>
    
    
      <category term="R" scheme="https://valerianiceria.github.io/tags/R/"/>
    
      <category term="Data Science" scheme="https://valerianiceria.github.io/tags/Data-Science/"/>
    
  </entry>
  
</feed>
